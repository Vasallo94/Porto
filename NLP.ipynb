{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP son las siglas de Procesamiento del Lenguaje Natural (Natural Language Processing en inglés). Es una rama de la inteligencia artificial que se centra en la interacción entre las computadoras y el lenguaje humano. En otras palabras, NLP es una forma para que las computadoras entiendan, interpreten y manipulen el lenguaje humano.\n",
    "\n",
    "El NLP combina la ciencia de la computación y la lingüística, por lo que las computadoras pueden interactuar con los textos humanos de una manera útil. Por ejemplo, el NLP se utiliza en los correctores ortográficos, en los sistemas de respuesta a preguntas (como los asistentes virtuales), en los traductores automáticos, en los sistemas de análisis de sentimientos, entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK, que significa Natural Language Toolkit, es una biblioteca líder en Python para el procesamiento del lenguaje natural (NLP). Fue desarrollada por investigadores y estudiantes en el campo de la lingüística y la ciencia de la computación para apoyar la enseñanza y el trabajo en NLP y en la lingüística computacional.\n",
    "\n",
    "NLTK proporciona interfaces fáciles de usar a más de 50 cuerpos y recursos léxicos, como WordNet, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificación, tokenización, derivación, etiquetado, análisis y razonamiento semántico.\n",
    "\n",
    "Algunos de los usos comunes de NLTK incluyen:\n",
    "\n",
    "- Tokenización: Divide el texto en frases o palabras.\n",
    "- Stemming y Lemmatization: Reduce las palabras a su raíz.\n",
    "- Etiquetado de partes del discurso (POS tagging): Identifica la parte de la oración de una palabra.\n",
    "- Análisis de sentimientos: Identifica la actitud del hablante.\n",
    "- Clasificación de texto: Clasifica el texto en diferentes categorías.\n",
    "- Extracción de información: Extrae información estructurada de texto no estructurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "# text mining\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from langdetect import detect\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Do not limit the width of columns\n",
    "pd.set_option('display.max_colwidth', None)  # Show the entire content of each cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('http://data.insideairbnb.com/portugal/norte/porto/2022-12-16/data/reviews.csv.gz', parse_dates=['date'])\n",
    "#df_reviews.to_csv('input/reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listing = pd.read_csv('http://data.insideairbnb.com/portugal/norte/porto/2022-12-16/visualisations/listings.csv', index_col= [\"id\"])\n",
    "#df_listing.to_csv('output/listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oporto_review = df_listing # Copio el df aquí para usar las review después en la sección de la Nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.merge(df_reviews, df_oporto_review[['host_id', 'host_name', 'name']], left_on = \"listing_id\", right_index=True, how = \"left\")\n",
    "df_reviews = df_reviews.reset_index(drop=True)\n",
    "df_reviews = df_reviews[['name', 'host_id', 'host_name', 'date', 'reviewer_id', 'reviewer_name', 'comments']]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que detecta el idioma de un texto (cuidado que tarda ~14min en ejecutarse)\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# La aplicamos a la columna de comentarios\n",
    "df_reviews['language'] = df_reviews['comments'].apply(detect_language)\n",
    "\n",
    "\n",
    "# Guardo este notebook porque tarda 14 minutos en detectar los idiomas de los comentarios.\n",
    "df_reviews.to_csv('output/df_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el archivo con los idiomas detectados\n",
    "df_reviews = pd.read_csv('output/df_reviews.csv', low_memory=False)\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos con plotly la columna de language para ver cuántos comentarios hay en cada idioma\n",
    "fig = px.histogram(df_reviews, x='language')\n",
    "fig.update_layout(title_text='Número de comentarios por idioma')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "# The purpose of this code is to handle SSL certificate verification when making HTTPS requests. By default, Python's ssl module verifies the SSL certificates of the websites it connects to. However, in some cases, such as when working with self-signed certificates or in development environments, it may be necessary to disable certificate verification.\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame to avoid warnings\n",
    "df_reviews = df_reviews.copy()\n",
    "\n",
    "# take out empty comments (530)\n",
    "df_reviews = df_reviews[df_reviews['comments'].notnull()]\n",
    "\n",
    "# remove non-alphanumeric characters and make everything lowercase\n",
    "df_reviews.loc[:, 'comments'] = df_reviews['comments'].str.replace('[^a-zA-Z0-9\\sáéíóúÁÉÍÓÚñÑçÇ~]+', '').str.lower()\n",
    "\n",
    "# remove windows new line\n",
    "df_reviews.loc[:, 'comments'] = df_reviews['comments'].str.replace('\\r\\n', \"\")\n",
    "\n",
    "# remove stopwords (from nltk library)\n",
    "stop_words = []\n",
    "for language in [\"english\", \"spanish\", \"french\", \"portuguese\", \"german\", \"italian\"]:\n",
    "    stop_words.extend(stopwords.words(language))\n",
    "df_reviews.loc[:, 'comments'] = df_reviews['comments'].apply(lambda x: \" \".join([i for i in x.split() if i not in stop_words]))\n",
    "\n",
    "# remove punctuation and replace x spaces by one space\n",
    "df_reviews.loc[:, 'comments'] = df_reviews['comments'].str.replace('[^\\w\\s]', \" \").str.replace('\\s+', ' ')\n",
    "\n",
    "df_reviews.comments.values[2]  # print same comments again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_reviews.comments.tolist()\n",
    "\n",
    "vec = CountVectorizer().fit(texts)\n",
    "bag_of_words = vec.transform(texts)\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "cvec_df = pd.DataFrame.from_records(words_freq, columns= ['words', 'counts']).sort_values(by=\"counts\", ascending=False)\n",
    "cvec_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = ['br', 'us', 'die', 'made']\n",
    "cvec_df = pd.DataFrame.from_records(words_freq, columns=['words', 'counts']).sort_values(by=\"counts\", ascending=False)\n",
    "\n",
    "# Filtrar las palabras excluidas\n",
    "cvec_df = cvec_df[~cvec_df['words'].isin(excluded_words)]\n",
    "\n",
    "cvec_dict = dict(zip(cvec_df.words, cvec_df.counts))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=cvec_dict)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('img/wordcloud.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
